{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "args = {\n",
    "    \"run_id\": \"1\",\n",
    "    \"data_path\": \"data/2024-10-13_PC1D_process10_data.pkl\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"n_samples\": 1000,\n",
    "    \"epochs\": 1000,\n",
    "    \"lr\": 1e-3,\n",
    "    \"hidden_dim\": 10,\n",
    "    \"step\": 50,\n",
    "    \"encoder_hidden_dim\": 128,\n",
    "    \"encoder_latent_dim\": 10,\n",
    "    \"encoder_path\": \"encoder_run_4\",\n",
    "    \"material_model\": \"m_dependent_b\",\n",
    "}\n",
    "# args = torch.load(\"material_model_run_1/args.pkl\")\n",
    "\n",
    "mm = importlib.import_module(args.material_model)\n",
    "from util import LossFunction\n",
    "from m_encoder import *\n",
    "\n",
    "device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(args.data_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "N = args.n_samples\n",
    "step = args.step\n",
    "\n",
    "e = torch.tensor(data[\"strain\"][:N, ::step], dtype=torch.float32).to(device)\n",
    "e_dot = torch.tensor(data[\"strain_rate\"][:N, ::step], dtype=torch.float32).to(device)\n",
    "s = torch.tensor(data[\"stress\"][:N, ::step], dtype=torch.float32).to(device)\n",
    "E = torch.tensor(data[\"E\"][:N], dtype=torch.float32).to(device)\n",
    "nu = torch.tensor(data[\"nu\"][:N], dtype=torch.float32).to(device)\n",
    "\n",
    "loss_function = LossFunction()\n",
    "\n",
    "ae_E = AutoEncoder(E.shape[1], args.encoder_hidden_dim, args.encoder_latent_dim).to(\n",
    "    device\n",
    ")\n",
    "ae_nu = AutoEncoder(nu.shape[1], args.encoder_hidden_dim, args.encoder_latent_dim).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "ae_E.load_state_dict(torch.load(f\"{args.encoder_path}/ae_E.pth\", weights_only=True))\n",
    "ae_nu.load_state_dict(torch.load(f\"{args.encoder_path}/ae_nu.pth\", weights_only=True))\n",
    "\n",
    "energy_input_dim = args.encoder_latent_dim * 2 + 2\n",
    "energy_hidden_dim = args.hidden_dim\n",
    "dissipation_input_dim = args.encoder_latent_dim * 2 + 2\n",
    "dissipation_hidden_dim = args.hidden_dim\n",
    "\n",
    "vmm = mm.ViscoelasticMaterialModelM(\n",
    "    energy_input_dim,\n",
    "    energy_hidden_dim,\n",
    "    dissipation_input_dim,\n",
    "    dissipation_hidden_dim,\n",
    "    ae_E.encoder,\n",
    "    ae_nu.encoder,\n",
    ").to(device)\n",
    "optimizer_m = torch.optim.Adam(vmm.parameters(), lr=args.lr)\n",
    "loss_history_m = []\n",
    "\n",
    "epochs = args.epochs\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss = mm.train_step_M(vmm, optimizer_m, e, e_dot, E, nu, s)\n",
    "    loss_history_m.append(loss)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
